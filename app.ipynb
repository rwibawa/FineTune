{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Prerequisite. Run on Terminal. Install miniconda. Navigate to the folder cd /home/asus/\n",
    "\n",
    "# mkdir -p ~/miniconda3\n",
    "# wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh\n",
    "# bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3\n",
    "# rm -rf ~/miniconda3/miniconda.sh\n",
    "# ~/miniconda3/bin/conda init bash\n",
    "# ~/miniconda3/bin/conda init zsh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### RUn on terminal: Create a virtual environment, Install all the requirements like pytorch, xformers, cuda etc.\n",
    "#### install the unsloth library and other libraries viz. peft\n",
    "\n",
    "# conda create --name unsloth_env python=3.11 pytorch-cuda=12.1 pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers -y\n",
    "# conda activate unsloth_env\n",
    "# pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "# pip install --no-deps trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit\",\n",
    "    \"unsloth/DeepSeek-R1-Distill-Qwen-14B-unsloth-bnb-4bit\",\n",
    "    \"unsloth/DeepSeek-R1-Distill-Qwen-7B-unsloth-bnb-4bit\",\n",
    "    \"unsloth/phi-4-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Mistral-Small-24B-Instruct-2501-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Llama-3.2-3B-Instruct-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Llama-3.2-1B-Instruct-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Mistral-Small-24B-Base-2501-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Llama-3.1-8B-Instruct-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Llama-3.1-8B-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Llama-3.2-11B-Vision-Instruct-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Qwen2.5-VL-7B-Instruct-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Qwen2.5-VL-72B-Instruct-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Qwen2.5-VL-3B-Instruct-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Qwen2-VL-7B-Instruct-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Qwen2-VL-2B-Instruct-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Pixtral-12B-2409-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Llama-3.2-11B-Vision-unsloth-bnb-4bit\",\n",
    "    \"unsloth/QwQ-32B-Preview-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Qwen2.5-3B-Instruct-unsloth-bnb-4bit\",\n",
    "] # More models at https://huggingface.co/unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.2.15: Fast Qwen2 patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4070. Max memory: 11.994 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1. CUDA: 8.9. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m FastLanguageModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      2\u001b[0m     model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsloth/DeepSeek-R1-Distill-Qwen-7B-unsloth-bnb-4bit\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     max_seq_length \u001b[38;5;241m=\u001b[39m max_seq_length,\n\u001b[1;32m      4\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtype,\n\u001b[1;32m      5\u001b[0m     load_in_4bit \u001b[38;5;241m=\u001b[39m load_in_4bit,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\u001b[39;00m\n\u001b[1;32m      7\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env2/lib/python3.11/site-packages/unsloth/models/loader.py:296\u001b[0m, in \u001b[0;36mFastLanguageModel.from_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, use_exact_model_name, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, *args, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 296\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m dispatch_model\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    297\u001b[0m     model_name        \u001b[38;5;241m=\u001b[39m model_name,\n\u001b[1;32m    298\u001b[0m     max_seq_length    \u001b[38;5;241m=\u001b[39m max_seq_length,\n\u001b[1;32m    299\u001b[0m     dtype             \u001b[38;5;241m=\u001b[39m _get_dtype(dtype),\n\u001b[1;32m    300\u001b[0m     load_in_4bit      \u001b[38;5;241m=\u001b[39m load_in_4bit,\n\u001b[1;32m    301\u001b[0m     token             \u001b[38;5;241m=\u001b[39m token,\n\u001b[1;32m    302\u001b[0m     device_map        \u001b[38;5;241m=\u001b[39m device_map,\n\u001b[1;32m    303\u001b[0m     rope_scaling      \u001b[38;5;241m=\u001b[39m rope_scaling,\n\u001b[1;32m    304\u001b[0m     fix_tokenizer     \u001b[38;5;241m=\u001b[39m fix_tokenizer,\n\u001b[1;32m    305\u001b[0m     model_patcher     \u001b[38;5;241m=\u001b[39m dispatch_model,\n\u001b[1;32m    306\u001b[0m     tokenizer_name    \u001b[38;5;241m=\u001b[39m tokenizer_name,\n\u001b[1;32m    307\u001b[0m     trust_remote_code \u001b[38;5;241m=\u001b[39m trust_remote_code,\n\u001b[1;32m    308\u001b[0m     revision          \u001b[38;5;241m=\u001b[39m revision \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_peft \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m \n\u001b[1;32m    310\u001b[0m     fast_inference    \u001b[38;5;241m=\u001b[39m fast_inference,\n\u001b[1;32m    311\u001b[0m     gpu_memory_utilization \u001b[38;5;241m=\u001b[39m gpu_memory_utilization,\n\u001b[1;32m    312\u001b[0m     float8_kv_cache   \u001b[38;5;241m=\u001b[39m float8_kv_cache,\n\u001b[1;32m    313\u001b[0m     random_state      \u001b[38;5;241m=\u001b[39m random_state,\n\u001b[1;32m    314\u001b[0m     max_lora_rank     \u001b[38;5;241m=\u001b[39m max_lora_rank,\n\u001b[1;32m    315\u001b[0m     disable_log_stats \u001b[38;5;241m=\u001b[39m disable_log_stats,\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    317\u001b[0m )\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resize_model_vocab \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    320\u001b[0m     model\u001b[38;5;241m.\u001b[39mresize_token_embeddings(resize_model_vocab)\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env2/lib/python3.11/site-packages/unsloth/models/qwen2.py:87\u001b[0m, in \u001b[0;36mFastQwen2Model.from_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_pretrained\u001b[39m(\n\u001b[1;32m     74\u001b[0m     model_name        \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen/Qwen2-7B\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     86\u001b[0m ):\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m FastLlamaModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     88\u001b[0m         model_name        \u001b[38;5;241m=\u001b[39m model_name,\n\u001b[1;32m     89\u001b[0m         max_seq_length    \u001b[38;5;241m=\u001b[39m max_seq_length,\n\u001b[1;32m     90\u001b[0m         dtype             \u001b[38;5;241m=\u001b[39m dtype,\n\u001b[1;32m     91\u001b[0m         load_in_4bit      \u001b[38;5;241m=\u001b[39m load_in_4bit,\n\u001b[1;32m     92\u001b[0m         token             \u001b[38;5;241m=\u001b[39m token,\n\u001b[1;32m     93\u001b[0m         device_map        \u001b[38;5;241m=\u001b[39m device_map,\n\u001b[1;32m     94\u001b[0m         rope_scaling      \u001b[38;5;241m=\u001b[39m rope_scaling,\n\u001b[1;32m     95\u001b[0m         fix_tokenizer     \u001b[38;5;241m=\u001b[39m fix_tokenizer,\n\u001b[1;32m     96\u001b[0m         model_patcher     \u001b[38;5;241m=\u001b[39m FastQwen2Model,\n\u001b[1;32m     97\u001b[0m         tokenizer_name    \u001b[38;5;241m=\u001b[39m tokenizer_name,\n\u001b[1;32m     98\u001b[0m         trust_remote_code \u001b[38;5;241m=\u001b[39m trust_remote_code,\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    100\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env2/lib/python3.11/site-packages/unsloth/models/llama.py:1796\u001b[0m, in \u001b[0;36mFastLlamaModel.from_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, **kwargs)\u001b[0m\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_in_4bit: kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m bnb_config\n\u001b[1;32m   1795\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fast_inference:\n\u001b[0;32m-> 1796\u001b[0m     model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m   1797\u001b[0m         model_name,\n\u001b[1;32m   1798\u001b[0m         device_map              \u001b[38;5;241m=\u001b[39m device_map,\n\u001b[1;32m   1799\u001b[0m         torch_dtype             \u001b[38;5;241m=\u001b[39m dtype,\n\u001b[1;32m   1800\u001b[0m         \u001b[38;5;66;03m# quantization_config     = bnb_config,\u001b[39;00m\n\u001b[1;32m   1801\u001b[0m         token                   \u001b[38;5;241m=\u001b[39m token,\n\u001b[1;32m   1802\u001b[0m         max_position_embeddings \u001b[38;5;241m=\u001b[39m max_position_embeddings,\n\u001b[1;32m   1803\u001b[0m         trust_remote_code       \u001b[38;5;241m=\u001b[39m trust_remote_code,\n\u001b[1;32m   1804\u001b[0m         attn_implementation     \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meager\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1805\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1806\u001b[0m     )\n\u001b[1;32m   1807\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1808\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munsloth_zoo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvllm_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m   1809\u001b[0m         load_vllm,\n\u001b[1;32m   1810\u001b[0m         get_vllm_state_dict,\n\u001b[1;32m   1811\u001b[0m         convert_vllm_to_huggingface,\n\u001b[1;32m   1812\u001b[0m         generate_batches,\n\u001b[1;32m   1813\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env2/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    565\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    566\u001b[0m     )\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env2/lib/python3.11/site-packages/transformers/modeling_utils.py:262\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env2/lib/python3.11/site-packages/transformers/modeling_utils.py:4262\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4259\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m infer_auto_device_map(model, dtype\u001b[38;5;241m=\u001b[39mtarget_dtype, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdevice_map_kwargs)\n\u001b[1;32m   4261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4262\u001b[0m         hf_quantizer\u001b[38;5;241m.\u001b[39mvalidate_environment(device_map\u001b[38;5;241m=\u001b[39mdevice_map)\n\u001b[1;32m   4264\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4265\u001b[0m     model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env2/lib/python3.11/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:103\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m--> 103\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    104\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    105\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    106\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    107\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`from_pretrained`. Check \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    108\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    109\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor more details. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    110\u001b[0m         )\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbitsandbytes\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;241m<\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.39.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have a version of `bitsandbytes` that is not compatible with 4bit inference and training\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you have the latest version of `bitsandbytes` installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    116\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/DeepSeek-R1-Distill-Qwen-7B-unsloth-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.2.15 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt= \"\"\"Below is an instruction that give an sql prompt. Write a response that appropriately completes the request and gives you an sql and the corresponding explanation.\n",
    "\n",
    "### sql_prompt:\n",
    "{}\n",
    "\n",
    "### sql:\n",
    "{}\n",
    "\n",
    "### Explanation:\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Below is an instruction that give an sql prompt. Write a response that appropriately completes the request and gives you an sql and the corresponding explanation.\\n\\n### sql_prompt:\\n{}\\n\\n### sql:\\n{}\\n\\n### Explanation:\\n{}'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpaca_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "    sql_prompt = examples[\"sql_prompt\"]\n",
    "    sql = examples[\"sql\"]\n",
    "    sql_explanation = examples['sql_explanation']\n",
    "\n",
    "    texts = []\n",
    "    \n",
    "    EOS_TOKEN = tokenizer.eos_token\n",
    "    for sql_prompt, sql, sql_explanation in zip(sql_prompt, sql, sql_explanation):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(sql_prompt, sql, sql_explanation) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'domain', 'domain_description', 'sql_complexity', 'sql_complexity_description', 'sql_task_type', 'sql_task_type_description', 'sql_prompt', 'sql_context', 'sql', 'sql_explanation']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"gretelai/synthetic_text_to_sql\", split = \"train\")\n",
    "print(dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'domain', 'domain_description', 'sql_complexity', 'sql_complexity_description', 'sql_task_type', 'sql_task_type_description', 'sql_prompt', 'sql_context', 'sql', 'sql_explanation'],\n",
       "    num_rows: 100000\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edf843de49da4816a64d2089cb5baa95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'domain', 'domain_description', 'sql_complexity', 'sql_complexity_description', 'sql_task_type', 'sql_task_type_description', 'sql_prompt', 'sql_context', 'sql', 'sql_explanation', 'text'],\n",
       "    num_rows: 100000\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Below is an instruction that give an sql prompt. Write a response that appropriately completes the request and gives you an sql and the corresponding explanation.\\n\\n### sql_prompt:\\nWhat is the total volume of timber sold by each salesperson, sorted by salesperson?\\n\\n### sql:\\nSELECT salesperson_id, name, SUM(volume) as total_volume FROM timber_sales JOIN salesperson ON timber_sales.salesperson_id = salesperson.salesperson_id GROUP BY salesperson_id, name ORDER BY total_volume DESC;\\n\\n### Explanation:\\nJoins timber_sales and salesperson tables, groups sales by salesperson, calculates total volume sold by each salesperson, and orders the results by total volume in descending order.<｜end▁of▁sentence｜>',\n",
       " 'Below is an instruction that give an sql prompt. Write a response that appropriately completes the request and gives you an sql and the corresponding explanation.\\n\\n### sql_prompt:\\nList all the unique equipment types and their corresponding total maintenance frequency from the equipment_maintenance table.\\n\\n### sql:\\nSELECT equipment_type, SUM(maintenance_frequency) AS total_maintenance_frequency FROM equipment_maintenance GROUP BY equipment_type;\\n\\n### Explanation:\\nThis query groups the equipment_maintenance table by equipment_type and calculates the sum of maintenance_frequency for each group, then returns the equipment_type and the corresponding total_maintenance_frequency.<｜end▁of▁sentence｜>',\n",
       " \"Below is an instruction that give an sql prompt. Write a response that appropriately completes the request and gives you an sql and the corresponding explanation.\\n\\n### sql_prompt:\\nHow many marine species are found in the Southern Ocean?\\n\\n### sql:\\nSELECT COUNT(*) FROM marine_species WHERE location = 'Southern Ocean';\\n\\n### Explanation:\\nThis query counts the number of marine species found in the Southern Ocean by using the COUNT() function with a WHERE clause.<｜end▁of▁sentence｜>\",\n",
       " 'Below is an instruction that give an sql prompt. Write a response that appropriately completes the request and gives you an sql and the corresponding explanation.\\n\\n### sql_prompt:\\nWhat is the total trade value and average price for each trader and stock in the trade_history table?\\n\\n### sql:\\nSELECT trader_id, stock, SUM(price * quantity) as total_trade_value, AVG(price) as avg_price FROM trade_history GROUP BY trader_id, stock;\\n\\n### Explanation:\\nThis query calculates the total trade value and average price for each trader and stock by grouping the trade_history table by the trader_id and stock columns and summing up the product of price and quantity for each trade and getting the average price for each group.<｜end▁of▁sentence｜>']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"text\"][0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2686c44d9c9f4fe7b57e46ecb98ae330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting train dataset to ChatML (num_proc=2):   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f9a364f4e234dcfbc04aa2736fe1b13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset (num_proc=2):   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "904ddcb14b464bec85bcdb860a9e3f39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=2):   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d49013437984bedbd566fdedbbc68b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=2):   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 2,\n",
    "        warmup_steps = 3,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps =  60,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 4070. Max memory = 11.994 GB.\n",
      "8.143 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 100,000 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient Accumulation steps = 2\n",
      "\\        /    Total batch size = 2 | Total steps = 60\n",
      " \"-____-\"     Number of trainable parameters = 40,370,176\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 01:03, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.151200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.050800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.273700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.721100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.754600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.602500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.528800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.678500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.780100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.683100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.328500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.579200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.098700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.373000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.075600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.977600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.717000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.068000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.052400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.073200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.171200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.051800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.130000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.195700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.960400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.743600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.802100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.169600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.171800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.837200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.914400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.687900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.880700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.749900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.976600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.807800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.835000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.032100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.942200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.774800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.701000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.786100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.813300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.709400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.023300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.068000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.836000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.726000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.928100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.783600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.945500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.644000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.744500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.974500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.539000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.822000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.685800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.835800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.894000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68.7814 seconds used for training.\n",
      "1.15 minutes used for training.\n",
      "Peak reserved memory = 8.764 GB.\n",
      "Peak reserved memory for training = 0.621 GB.\n",
      "Peak reserved memory % of max memory = 73.07 %.\n",
      "Peak reserved memory for training % of max memory = 5.178 %.\n"
     ]
    }
   ],
   "source": [
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('lora_model/tokenizer_config.json',\n",
       " 'lora_model/special_tokens_map.json',\n",
       " 'lora_model/tokenizer.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"lora_model\") # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: ##### The current model auto adds a BOS token.\n",
      "Unsloth: ##### Your chat template has a BOS token. We shall remove it temporarily.\n",
      "Makefile:2: *** The Makefile build is deprecated. Use the CMake build instead. For more details, see https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md.  Stop.\n",
      "sh: 1: cmake: not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: Entering directory '/home/ryan/Documents/workspaces/workspace_ai/FineTune/llama.cpp'\n",
      "make: Leaving directory '/home/ryan/Documents/workspaces/workspace_ai/FineTune/llama.cpp'\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "*** Unsloth: Failed compiling llama.cpp using os.system(...) with error 32512. Please report this ASAP!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained_gguf(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, tokenizer, quantization_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env2/lib/python3.11/site-packages/unsloth/save.py:1697\u001b[0m, in \u001b[0;36munsloth_save_pretrained_gguf\u001b[0;34m(self, save_directory, tokenizer, quantization_method, first_conversion, push_to_hub, token, private, is_main_process, state_dict, save_function, max_shard_size, safe_serialization, variant, save_peft_format, tags, temporary_location, maximum_memory_usage)\u001b[0m\n\u001b[1;32m   1695\u001b[0m python_install \u001b[38;5;241m=\u001b[39m install_python_non_blocking([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgguf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprotobuf\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   1696\u001b[0m git_clone\u001b[38;5;241m.\u001b[39mwait()\n\u001b[0;32m-> 1697\u001b[0m makefile \u001b[38;5;241m=\u001b[39m install_llama_cpp_make_non_blocking()\n\u001b[1;32m   1698\u001b[0m new_save_directory, old_username \u001b[38;5;241m=\u001b[39m unsloth_save_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39marguments)\n\u001b[1;32m   1699\u001b[0m python_install\u001b[38;5;241m.\u001b[39mwait()\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env2/lib/python3.11/site-packages/unsloth/save.py:783\u001b[0m, in \u001b[0;36minstall_llama_cpp_make_non_blocking\u001b[0;34m()\u001b[0m\n\u001b[1;32m    781\u001b[0m check \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcmake llama.cpp -B llama.cpp/build -DBUILD_SHARED_LIBS=OFF -DGGML_CUDA=OFF -DLLAMA_CURL=ON\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 783\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*** Unsloth: Failed compiling llama.cpp using os.system(...) with error \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheck\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Please report this ASAP!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    785\u001b[0m \u001b[38;5;66;03m# f\"cmake --build llama.cpp/build --config Release -j{psutil.cpu_count()*2} --clean-first --target {' '.join(LLAMA_CPP_TARGETS)}\",\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: *** Unsloth: Failed compiling llama.cpp using os.system(...) with error 32512. Please report this ASAP!"
     ]
    }
   ],
   "source": [
    "# model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow this Steps for Local Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 28.16 out of 49.39 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█████▊                                                                                                                                                           | 1/28 [00:00<00:09,  2.71it/s]\n",
      "We will save to Disk and not RAM now.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28/28 [00:17<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained_merged(\"merged_model\", tokenizer, save_method = \"merged_16bit\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'llama.cpp' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone --recursive https://github.com/ggerganov/llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: Entering directory '/home/ryan/Documents/workspaces/workspace_ai/FineTune/llama.cpp'\n",
      "Makefile:2: *** The Makefile build is deprecated. Use the CMake build instead. For more details, see https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md.  Stop.\n",
      "make: Leaving directory '/home/ryan/Documents/workspaces/workspace_ai/FineTune/llama.cpp'\n"
     ]
    }
   ],
   "source": [
    "!make clean -C llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: Entering directory '/home/ryan/Documents/workspaces/workspace_ai/FineTune/llama.cpp'\n",
      "Makefile:2: *** The Makefile build is deprecated. Use the CMake build instead. For more details, see https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md.  Stop.\n",
      "make: Leaving directory '/home/ryan/Documents/workspaces/workspace_ai/FineTune/llama.cpp'\n"
     ]
    }
   ],
   "source": [
    "!make all -j -C llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gguf in /home/ryan/miniconda3/envs/unsloth_env2/lib/python3.11/site-packages (0.14.0)\n",
      "Requirement already satisfied: protobuf in /home/ryan/miniconda3/envs/unsloth_env2/lib/python3.11/site-packages (3.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ryan/miniconda3/envs/unsloth_env2/lib/python3.11/site-packages (from gguf) (2.2.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ryan/miniconda3/envs/unsloth_env2/lib/python3.11/site-packages (from gguf) (6.0.2)\n",
      "Requirement already satisfied: sentencepiece<=0.2.0,>=0.1.98 in /home/ryan/miniconda3/envs/unsloth_env2/lib/python3.11/site-packages (from gguf) (0.2.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ryan/miniconda3/envs/unsloth_env2/lib/python3.11/site-packages (from gguf) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install gguf protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:hf-to-gguf:Loading model: merged_model\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:token_embd.weight,         torch.bfloat16 --> Q8_0, shape = {3584, 152064}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,  torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,  torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,  torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,  torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,  torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,  torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,  torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,  torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,  torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,  torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00003-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.24.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.25.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.26.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.27.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:output_norm.weight,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00004-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:output.weight,             torch.bfloat16 --> Q8_0, shape = {3584, 152064}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 131072\n",
      "INFO:hf-to-gguf:gguf: embedding length = 3584\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 18944\n",
      "INFO:hf-to-gguf:gguf: head count = 28\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 4\n",
      "INFO:hf-to-gguf:gguf: rope theta = 10000\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\n",
      "INFO:hf-to-gguf:gguf: file type = 7\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:gguf.vocab:Adding 151387 merge(s).\n",
      "INFO:gguf.vocab:Setting special token type bos to 151646\n",
      "INFO:gguf.vocab:Setting special token type eos to 151643\n",
      "INFO:gguf.vocab:Setting special token type pad to 151654\n",
      "INFO:gguf.vocab:Setting add_bos_token to True\n",
      "INFO:gguf.vocab:Setting add_eos_token to False\n",
      "INFO:gguf.vocab:Setting chat_template to {% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}{%- if message['role'] == 'system' %}{% set ns.system_prompt = message['content'] %}{%- endif %}{%- endfor %}{{ns.system_prompt}}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<｜User｜>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is none %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls']%}{%- if not ns.is_first %}{{'<｜Assistant｜><｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{%- set ns.is_first = true -%}{%- else %}{{'\\n' + '<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{{'<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'}}{%- endif %}{%- endfor %}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is not none %}{%- if ns.is_tool %}{{'<｜tool▁outputs▁end｜>' + message['content'] + '<｜end▁of▁sentence｜>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\\n<｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<｜tool▁outputs▁end｜>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<｜Assistant｜><think>\\n'}}{% endif %}\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:text2sql_8.gguf: n_tensors = 339, total_size = 8.1G\n",
      "/home/ryan/Documents/workspaces/workspace_ai/FineTune/llama.cpp/gguf-py/gguf/quants.py:387: RuntimeWarning: overflow encountered in divide\n",
      "  id = np.where(d == 0, 0, 1 / d)\n",
      "/home/ryan/Documents/workspaces/workspace_ai/FineTune/llama.cpp/gguf-py/gguf/quants.py:46: RuntimeWarning: invalid value encountered in subtract\n",
      "  b = floored + np.floor(2 * (a - floored))\n",
      "/home/ryan/Documents/workspaces/workspace_ai/FineTune/llama.cpp/gguf-py/gguf/quants.py:393: RuntimeWarning: invalid value encountered in cast\n",
      "  qs = qs.astype(np.int8).view(np.uint8)\n",
      "/home/ryan/Documents/workspaces/workspace_ai/FineTune/llama.cpp/gguf-py/gguf/quants.py:388: RuntimeWarning: invalid value encountered in multiply\n",
      "  qs = np_roundf(blocks * id)\n",
      "Writing: 100%|███████████████████████████| 8.09G/8.09G [01:01<00:00, 131Mbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to text2sql_8.gguf\n"
     ]
    }
   ],
   "source": [
    "!python llama.cpp/convert_hf_to_gguf.py merged_model --outfile text2sql_8.gguf --outtype q8_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSTALL OLLAMA using Terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Installing ollama to /usr/local\n",
      "[sudo] password for ryan: "
     ]
    }
   ],
   "source": [
    "!curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components \u001b[K\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 0% ⠋ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 0% ⠙ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 0% ⠹ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 1% ⠸ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 1% ⠼ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 2% ⠴ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 2% ⠦ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 2% ⠧ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 3% ⠇ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 3% ⠏ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 4% ⠋ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 4% ⠙ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 4% ⠹ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 4% ⠸ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 5% ⠼ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 5% ⠴ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 6% ⠦ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 6% ⠧ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 7% ⠇ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 7% ⠏ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 7% ⠋ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 8% ⠙ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 8% ⠹ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 8% ⠸ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 9% ⠼ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 9% ⠴ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 10% ⠦ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 10% ⠧ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 10% ⠇ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 11% ⠏ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 11% ⠋ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 11% ⠙ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 12% ⠹ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 12% ⠸ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 12% ⠼ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 13% ⠴ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 13% ⠦ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 14% ⠧ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 14% ⠇ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 14% ⠏ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 15% ⠋ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 15% ⠙ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 16% ⠹ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 16% ⠸ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 16% ⠼ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 17% ⠴ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 17% ⠦ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 18% ⠧ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 18% ⠇ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 18% ⠏ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 19% ⠋ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 19% ⠙ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 19% ⠹ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 20% ⠸ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 20% ⠼ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 20% ⠴ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 21% ⠦ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 21% ⠧ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 21% ⠇ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 22% ⠏ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 22% ⠋ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 23% ⠙ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 23% ⠹ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 23% ⠸ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 24% ⠼ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 24% ⠴ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 25% ⠦ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 25% ⠧ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 25% ⠇ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 26% ⠏ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 26% ⠋ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 26% ⠙ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 27% ⠹ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 27% ⠸ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 28% ⠼ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 28% ⠴ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 28% ⠦ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 29% ⠧ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 29% ⠇ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 29% ⠏ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 30% ⠋ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 30% ⠙ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 30% ⠹ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 31% ⠸ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 31% ⠼ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 31% ⠴ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 32% ⠦ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 32% ⠧ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 33% ⠇ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 33% ⠏ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 33% ⠋ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 34% ⠙ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 34% ⠹ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 35% ⠸ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 35% ⠼ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 35% ⠴ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 36% ⠦ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 36% ⠧ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 36% ⠇ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 37% ⠏ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 37% ⠋ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 37% ⠙ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 38% ⠹ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 38% ⠸ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 38% ⠼ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 39% ⠴ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 39% ⠦ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 40% ⠧ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 40% ⠇ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 40% ⠏ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 41% ⠋ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 41% ⠙ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 41% ⠹ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 42% ⠸ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 42% ⠼ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 42% ⠴ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 43% ⠦ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 43% ⠧ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 44% ⠇ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 44% ⠏ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 44% ⠋ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 45% ⠙ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 45% ⠹ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 45% ⠸ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 46% ⠼ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 46% ⠴ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 46% ⠦ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 47% ⠧ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 47% ⠇ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 47% ⠏ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 48% ⠋ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 48% ⠙ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 48% ⠹ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 49% ⠸ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 49% ⠼ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 49% ⠴ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 50% ⠦ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 50% ⠧ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 51% ⠇ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 51% ⠏ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 51% ⠋ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 52% ⠙ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 52% ⠹ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 52% ⠸ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 53% ⠼ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 53% ⠴ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 53% ⠦ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 54% ⠧ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 54% ⠇ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 55% ⠏ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 55% ⠋ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 55% ⠙ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 56% ⠹ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 56% ⠸ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 56% ⠼ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 57% ⠴ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 57% ⠦ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 57% ⠧ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 58% ⠇ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 58% ⠏ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 58% ⠋ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 59% ⠙ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 59% ⠹ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 59% ⠸ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 60% ⠼ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 60% ⠴ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 60% ⠦ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 60% ⠧ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 61% ⠇ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 61% ⠏ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 61% ⠋ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 62% ⠙ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 62% ⠹ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 62% ⠸ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 63% ⠼ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 63% ⠴ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 63% ⠦ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 64% ⠧ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 64% ⠇ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 65% ⠏ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 65% ⠋ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 65% ⠙ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 66% ⠹ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 66% ⠸ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 66% ⠼ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 67% ⠴ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 67% ⠦ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 67% ⠧ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 68% ⠇ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 68% ⠏ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 68% ⠋ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 69% ⠙ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 69% ⠹ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 69% ⠸ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 70% ⠼ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 70% ⠴ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 70% ⠦ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 71% ⠧ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 71% ⠇ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 72% ⠏ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 72% ⠋ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 73% ⠙ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 73% ⠹ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 73% ⠸ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 74% ⠼ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 74% ⠴ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 74% ⠦ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 75% ⠧ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 75% ⠇ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 75% ⠏ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 76% ⠋ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 76% ⠙ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 76% ⠹ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 77% ⠸ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 77% ⠼ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 77% ⠴ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 78% ⠦ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 78% ⠧ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 78% ⠇ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 79% ⠏ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 79% ⠋ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 80% ⠙ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 80% ⠹ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 80% ⠸ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 81% ⠼ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 81% ⠴ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 81% ⠦ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 82% ⠧ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 82% ⠇ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 83% ⠏ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 83% ⠋ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 83% ⠙ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 83% ⠹ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 84% ⠸ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 84% ⠼ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 84% ⠴ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 85% ⠦ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 85% ⠧ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 86% ⠇ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 86% ⠏ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 86% ⠋ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 87% ⠙ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 87% ⠹ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 87% ⠸ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 87% ⠼ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 88% ⠴ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 88% ⠦ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 89% ⠧ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 89% ⠇ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 89% ⠏ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 90% ⠋ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 90% ⠙ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 90% ⠹ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 90% ⠸ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 91% ⠼ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 91% ⠴ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 91% ⠦ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 92% ⠧ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 92% ⠇ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 92% ⠏ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 93% ⠋ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 93% ⠙ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 93% ⠹ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 94% ⠸ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 94% ⠼ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 95% ⠴ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 95% ⠦ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 95% ⠧ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 96% ⠇ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 96% ⠏ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 97% ⠋ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 97% ⠙ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 97% ⠹ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 98% ⠸ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 98% ⠼ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 98% ⠴ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 98% ⠦ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 98% ⠧ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 99% ⠇ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 99% ⠏ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K935775f1f2aa9f54bd240b77ab342ab1a 99% ⠋ \u001b[K\u001b[?25h\u001b[?2026l\n",
      "copying file sha256:7ae6bfe1383cc7890779852f69c353f935775f1f2aa9f54bd240b77ab342ab1a 100% \u001b[K\n",
      "parsing GGUF ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:7ae6bfe1383cc7890779852f69c353f935775f1f2aa9f54bd240b77ab342ab1a 100% \u001b[K\n",
      "parsing GGUF \u001b[K\n",
      "using existing layer sha256:7ae6bfe1383cc7890779852f69c353f935775f1f2aa9f54bd240b77ab342ab1a \u001b[K\n",
      "creating new layer sha256:0b0806170c94d41762ced6ab61e14aca83ac2df90f2ff5a0bc1a6f0a96634fa0 \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "!ollama create unsloth_model_8 -f Modelfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt1= \"\"\"Below is an instruction that give an sql prompt. Write a response that appropriately completes the request and gives you an sql and the corresponding explanation.\n",
    "\n",
    "### sql_prompt:\n",
    "{}\n",
    "\n",
    "### sql:\n",
    "{}\n",
    "\n",
    "### Explanation:\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = alpaca_prompt1.format(\n",
    "    \"What is the total volume of timber sold by each salesperson, sorted by salesperson?\",\n",
    "    \"\",\n",
    "    \"\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Below is an instruction that give an sql prompt. Write a response that appropriately completes the request and gives you an sql and the corresponding explanation.\\n\\n### sql_prompt:\\nWhat is the total volume of timber sold by each salesperson, sorted by salesperson?\\n\\n### sql:\\n\\n\\n### Explanation:\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the average property size in inclusive housing areas?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = alpaca_prompt1.format(\n",
    "    query,\n",
    "    \"\",\n",
    "    \"\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Below is an instruction that give an sql prompt. Write a response that appropriately completes the request and gives you an sql and the corresponding explanation.\\n\\n### sql_prompt:\\nWhat is the average property size in inclusive housing areas?\\n\\n### sql:\\n\\n\\n### Explanation:\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
